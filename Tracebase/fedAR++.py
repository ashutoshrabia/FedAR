# -*- coding: utf-8 -*-
"""CS637.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-YegIywrgYITdftQqBomPR9UXx4fCdtJ
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import ast
from keras.optimizers import SGD

# Load the dataset
def load_data(file_path, max_length=30):
    data = pd.read_csv(file_path)
    data['Footprint'] = data['Footprint'].apply(lambda x: ast.literal_eval(x))

    # Pad sequences to ensure uniform length
    X = pad_sequences(data['Footprint'].tolist(), maxlen=max_length, padding='post', truncating='post')
    y = data['Label'].values
    return X, y

# Specify the file path
file_path = '/content/processed_dataset.csv'
X, y = load_data(file_path)

# Define a function to build the model
def build_model(input_shape, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Conv1D(128, 1, activation='relu', input_shape=input_shape),
        tf.keras.layers.Conv1D(128, 1, activation='relu'),
        tf.keras.layers.Conv1D(128, 1, activation='relu'),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    learning_rate = 0.1
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Define the training process with noise handling
def train_with_noise_handling(X, y, num_iterations=10, learning_rate=0.01):
    # Encode labels
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    # Split data into training and validation
    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    # Build the model
    model = build_model(input_shape=(X.shape[1], 1), num_classes=len(label_encoder.classes_))

    # Initial training and store history
    history = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=5, validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], 1), y_val))

    # Noise handling loop
    for iteration in range(num_iterations):
        # Step 2: Estimate label distributions
        label_distributions = model.predict(X_train.reshape(X_train.shape[0], X_train.shape[1], 1))

        # Update the training labels based on predictions
        y_train = np.argmax(label_distributions, axis=1)

        # Step 3: Fine-tune the model and store history
        history_iteration = model.fit(X_train.reshape(X_train.shape[0], X_train.shape[1], 1), y_train, epochs=5, validation_data=(X_val.reshape(X_val.shape[0], X_val.shape[1], 1), y_val))

        # Combine histories
        history.history['loss'] += history_iteration.history['loss']
        history.history['accuracy'] += history_iteration.history['accuracy']
        history.history['val_loss'] += history_iteration.history['val_loss']
        history.history['val_accuracy'] += history_iteration.history['val_accuracy']

    return model, label_encoder, X_val, y_val, history

# Train the model
model, label_encoder, X_val, y_val, history = train_with_noise_handling(X, y)

# Function to evaluate the model
def evaluate_model(model, label_encoder, X_val, y_val):
    y_pred_probs = model.predict(X_val.reshape(X_val.shape[0], X_val.shape[1], 1))
    y_pred = np.argmax(y_pred_probs, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

# Evaluate the model
evaluate_model(model, label_encoder, X_val, y_val)

# Function to plot training history
def plot_history(history):
    # Plot training & validation accuracy values
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(loc='upper left')

    # Plot training & validation loss values
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(loc='upper left')

    plt.show()

# Plot training history
plot_history(history)

